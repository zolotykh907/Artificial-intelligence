{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 14 13:44:39 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-SXM2-32GB           Off | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   26C    P0              54W / 300W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2-32GB           Off | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   29C    P0              53W / 300W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2-32GB           Off | 00000000:B2:00.0 Off |                    0 |\n",
      "| N/A   25C    P0              52W / 300W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2-32GB           Off | 00000000:B3:00.0 Off |                    0 |\n",
      "| N/A   28C    P0              53W / 300W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = ['abraham_grampa_simpson', 'agnes_skinner', 'apu_nahasapeemapetilon', 'barney_gumble', \n",
    "'bart_simpson', 'carl_carlson', 'charles_montgomery_burns', 'chief_wiggum', 'cletus_spuckler', \n",
    "'comic_book_guy', 'disco_stu', 'edna_krabappel', 'fat_tony', 'gil', 'groundskeeper_willie', 'homer_simpson', \n",
    "'kent_brockman', 'krusty_the_clown', 'lenny_leonard', 'lionel_hutz', 'lisa_simpson', 'maggie_simpson', 'marge_simpson', \n",
    "'martin_prince', 'mayor_quimby', 'milhouse_van_houten', 'miss_hoover', 'moe_szyslak', 'ned_flanders', 'nelson_muntz', \n",
    "'otto_mann', 'patty_bouvier', 'principal_skinner', 'professor_john_frink', 'rainier_wolfcastle', 'ralph_wiggum', \n",
    "'selma_bouvier', 'sideshow_bob', 'sideshow_mel', 'snake_jailbird', 'troy_mcclure', 'waylon_smithers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_penultimate_folder(path):\n",
    "    folders = path.split('\\\\')\n",
    "    return folders[-2]\n",
    "\n",
    "# path = \"C:/Users/Professional/Desktop/MOP/simps/characters\\\\abraham_grampa_simpson\\\\pic_0000.jpg\"\n",
    "# penultimate_folder = get_penultimate_folder(path)\n",
    "# #print(penultimate_folder)\n",
    "\n",
    "def list_files_in_directory(directory):\n",
    "    i = 0\n",
    "    k = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            k.append(list([1, 1]))\n",
    "            k[i].insert(0, os.path.join(root, file))\n",
    "            k[i].insert(1, get_penultimate_folder(os.path.join(root, file)))\n",
    "            i = i+1\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# transformk = transforms.Compose([\n",
    "    # transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Изменение яркости и контраста\n",
    "    # transforms.GaussianBlur(kernel_size=3),  # Добавление гауссовского размытия\n",
    "    # transforms.RandomRotation(degrees=30),  # Случайный поворот изображения\n",
    "    # transforms.RandomHorizontalFlip(),  # Случайное отражение по горизонтали\n",
    "    # transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),  # Случайное масштабирование и обрезка\n",
    "    # transforms.RandomApply([transforms.Grayscale()], p=0.2),  # Применение черно-белого фильтра с вероятностью 0.2\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/i.zolotykh/Artificial-intelligence/jupiter_notebook\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 35\u001b[0m\n\u001b[1;32m     31\u001b[0m         label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(j\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_labels[index][\u001b[38;5;241m1\u001b[39m]), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n\u001b[0;32m---> 35\u001b[0m train_and_valid_set \u001b[38;5;241m=\u001b[39m \u001b[43mCustomImageDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msimpsons/simpsons_dataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 13\u001b[0m, in \u001b[0;36mCustomImageDataset.__init__\u001b[0;34m(self, img_dir, transform, target_transform)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img_dir, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, target_transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_labels \u001b[38;5;241m=\u001b[39m \u001b[43mlist_files_in_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transform\n",
      "Cell \u001b[0;32mIn[17], line 16\u001b[0m, in \u001b[0;36mlist_files_in_directory\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     14\u001b[0m         k\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlist\u001b[39m([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m     15\u001b[0m         k[i]\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, file))\n\u001b[0;32m---> 16\u001b[0m         k[i]\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m1\u001b[39m, \u001b[43mget_penultimate_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     17\u001b[0m         i \u001b[38;5;241m=\u001b[39m i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m k\n",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m, in \u001b[0;36mget_penultimate_folder\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_penultimate_folder\u001b[39m(path):\n\u001b[1;32m      2\u001b[0m     folders \u001b[38;5;241m=\u001b[39m path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfolders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pydicom, re\n",
    "# import tensorflow as tf\n",
    "from torch.utils.data import random_split\n",
    "from PIL import Image\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=0, target_transform=None):\n",
    "        self.img_labels = list_files_in_directory(img_dir)\n",
    "        self.transform = transform\n",
    "        #self.batch_size = 16\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(r'{}'.format(self.img_labels[index][0]))\n",
    "        w, h = image.size\n",
    "        if (self.transform == 0): \n",
    "            trans_loc = transforms.CenterCrop((min(w, h), min(w, h)))\n",
    "            image = trans_loc(image)\n",
    "            image = transform(image)\n",
    "            #print(\"hi\")\n",
    "        # else:\n",
    "            # image = transformk(image)\n",
    "        #print(image)\n",
    "        label = torch.tensor(j.index(self.img_labels[index][1]), dtype=torch.long)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "train_and_valid_set = CustomImageDataset(\"simpsons/simpsons_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20933 58 25\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_size = int(0.7 * len(train_and_valid_set))\n",
    "val_size = len(train_and_valid_set) - train_size\n",
    "train_dataset, val_dataset = random_split(train_and_valid_set, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "print(len(train_and_valid_set), len(train_dataloader), len(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = [0] * 42\n",
    "# print(len(p))\n",
    "# for imges, labels in train_dataloader:\n",
    "#     #labels = labels.tolist()\n",
    "#     for jo in labels:\n",
    "#         #print(j)\n",
    "#         p[jo] = p[jo]+1\n",
    "#     #print(p)\n",
    "#     #p = [0] * 42\n",
    "\n",
    "# print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.datasets import ImageFolder\n",
    "# directory = \"C:/Users/Professional/Desktop/MOP/simps/characters\"\n",
    "# classes = os.listdir(directory)\n",
    "\n",
    "# # transform = transforms.Compose([\n",
    "#     # transforms.Resize((128, 128)),\n",
    "#     # transforms.ToTensor(),\n",
    "#     # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  #преобразует пиксели из диапазона [0,1] в диапазон [-1,1]\n",
    "# # ])\n",
    "\n",
    "# data_dir = \"C:/Users/Professional/Desktop/MOP/simps/characters\"\n",
    "\n",
    "# dataset = ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "# train_size = int(0.8 * len(dataset))\n",
    "# val_size = len(dataset) - train_size\n",
    "# train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# print(\"Классы:\", dataset.classes)\n",
    "\n",
    "# print(len(dataset.classes))\n",
    "\n",
    "\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle = True)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size = 128, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "[269, 8, 183, 36, 404, 26, 351, 307, 16, 147, 3, 136, 9, 9, 37, 654, 149, 354, 96, 1, 413, 35, 426, 23, 62, 330, 6, 457, 441, 107, 6, 19, 338, 19, 15, 22, 25, 262, 9, 17, 0, 53, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# p = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,0, 0, 0,0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "# print(len(p))\n",
    "# for imges, labels in val_dataloader:\n",
    "#     #labels = labels.tolist()\n",
    "#     for jo in labels:\n",
    "#         #print(j)\n",
    "#         p[jo] = p[jo]+1\n",
    "#         #print(weight, hight),\n",
    "#     #labels = torch.tensor(labels)\n",
    "\n",
    "# print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from random import randrange\n",
    "# from torchsummary import summary\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class FullyConnectedNetwork(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "      super(FullyConnectedNetwork, self).__init__()\n",
    "      self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride = 1, padding=1)\n",
    "      #self.conv1_2 = nn.Conv2d(16, 16, kernel_size=3, stride = 1, padding=0)\n",
    "      self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1) \n",
    "      #self.conv2_2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=0) \n",
    "      self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "      #self.conv3_2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0)\n",
    "      self.pool = nn.MaxPool2d((2, 2), stride = 2) \n",
    "      self.fc1 = nn.Linear(65536, 1024) \n",
    "      self.fc2 = nn.Linear(1024, num_classes)\n",
    "      self.relu = nn.ReLU()\n",
    "      self.drop = nn.Dropout(p=0.5)\n",
    "      self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "      x = self.relu(self.conv1(x))\n",
    "      #print(\"help\")\n",
    "      x = self.pool(x) \n",
    "      x = self.relu(self.conv2(x))\n",
    "      x = self.pool(x) \n",
    "      x = self.relu(self.conv3(x))\n",
    "      x = self.pool(x)\n",
    "      x = self.drop(x)\n",
    "      x = self.relu(self.fc1(x.view(-1, 65536)))\n",
    "      x = self.fc2(x)\n",
    "      return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html\n",
    "\n",
    "def train_step(model, optimizer) -> float:\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_dataloader:\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        #print(output.shape, labels.shape)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss\n",
    "\n",
    "    with torch.no_grad():\n",
    "        train_loss = running_loss / len(train_dataloader)\n",
    "    return train_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_step(model) -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "\n",
    "    correct_total = 0.\n",
    "    running_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_dataloader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "            output = model(images)\n",
    "\n",
    "            prediction = output.max(dim=1,keepdim=True)[1] \n",
    "            correct_total += prediction.eq(labels.view_as(prediction)).sum()\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "            running_loss += loss\n",
    "\n",
    "    valid_loss = running_loss / len(val_dataloader)\n",
    "    accuracy = correct_total / len(val_dataloader.dataset)\n",
    "    return valid_loss.item(), accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train(epochs, model, optimizer):\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    valid_accs = []\n",
    "\n",
    "    for i in range(0, epochs):\n",
    "        start_time = time.time()\n",
    "        train_loss = train_step(model, optimizer)\n",
    "        valid_loss, valid_acc = valid_step(model)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_accs.append(valid_acc)\n",
    "        end_time = time.time()\n",
    "        print(i, \" - epoch, train loss = \", train_loss, \"val loss = \", valid_loss, \"time:\", end_time-start_time)\n",
    "        #pbar.set_description(f'Avg. train/valid loss: {train_loss:.4f}/{valid_loss:.4f}')\n",
    "    torch.save(\"C:/Users/Professional/Desktop/MOP/simps\", './model_weights_4.pth')\n",
    "    return train_losses, valid_losses, valid_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# torch.cuda.is_available()\n",
    "# #torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 16, 256, 256]             448\n",
      "              ReLU-2         [-1, 16, 256, 256]               0\n",
      "         MaxPool2d-3         [-1, 16, 128, 128]               0\n",
      "            Conv2d-4         [-1, 32, 128, 128]           4,640\n",
      "              ReLU-5         [-1, 32, 128, 128]               0\n",
      "         MaxPool2d-6           [-1, 32, 64, 64]               0\n",
      "            Conv2d-7           [-1, 64, 64, 64]          18,496\n",
      "              ReLU-8           [-1, 64, 64, 64]               0\n",
      "         MaxPool2d-9           [-1, 64, 32, 32]               0\n",
      "          Dropout-10           [-1, 64, 32, 32]               0\n",
      "           Linear-11                 [-1, 1024]      67,109,888\n",
      "             ReLU-12                 [-1, 1024]               0\n",
      "           Linear-13                   [-1, 42]          43,050\n",
      "================================================================\n",
      "Total params: 67,176,522\n",
      "Trainable params: 67,176,522\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 32.02\n",
      "Params size (MB): 256.26\n",
      "Estimated Total Size (MB): 289.02\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "model = FullyConnectedNetwork(num_classes=42).cuda()\n",
    "summary(model, (3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FullyConnectedNetwork(num_classes=42).cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "stats2 = train(epochs = 10, model=model, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Печать версии PyTorch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Печать информации о поддержке CUDA\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "\n",
    "# Печать версии CUDA (если доступно)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "\n",
    "# Печать версии cuDNN (если доступно)\n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version() if torch.cuda.is_available() else \"N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "plt.plot(stats2[0][:], label='train')\n",
    "plt.plot(stats2[1][:], label='valid')\n",
    "plt.legend()\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "#Text(0, 0.5, 'Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in stats2[3][:]:\n",
    "print(stats2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import random_split\n",
    "from tqdm import tqdm_gui\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "transform = transforms.Compose([\n",
    "      transforms.Resize((256, 256)),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "  ])\n",
    "\n",
    "directory = \"C:/Users/Professional/Desktop/MOP/simps/kaggle_simpson_testset/kaggle_simpson_testset/\"\n",
    "\n",
    "test_classes = []\n",
    "test_images = []\n",
    "\n",
    "def del_digit(img_name):\n",
    "  for char in img_name:\n",
    "    if char.isdigit():\n",
    "      img_name = img_name.replace(char, '')\n",
    "\n",
    "  clas = img_name[:-5]\n",
    "  test_classes.append(j.index(clas))\n",
    "\n",
    "test_imgs = os.listdir(directory)\n",
    "for img in test_imgs:\n",
    "  image_path = directory + img\n",
    "  image = Image.open(image_path)\n",
    "  y, x = image.size\n",
    "  trew = transforms.CenterCrop((min(x, y), min(x, y)))\n",
    "  image = trew(image)\n",
    "  test_images.append(transform(image).unsqueeze(0))\n",
    "  del_digit(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [0]*42\n",
    "print(len(p))\n",
    "for imges in test_classes:\n",
    "    p[imges] = p[imges]+1\n",
    "\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "true = 0\n",
    "total = len(test_images)\n",
    "\n",
    "tp = [0]*42\n",
    "fp = [0]*42\n",
    "\n",
    "for image, label in zip(test_images, test_classes):\n",
    "    outputs = model(image.cuda())\n",
    "    if torch.argmax(outputs) == label:\n",
    "      true = true+1\n",
    "      tp[label] = tp[label]+1\n",
    "    if torch.argmax(outputs) != label:\n",
    "      fp[torch.argmax(outputs)] = fp[torch.argmax(outputs)] + 1\n",
    "\n",
    "# for images, labels in val_dataloader:\n",
    "    # images, labels = images.cuda(), labels.cuda()\n",
    "    # output = model(images)\n",
    "    # output = output.max(dim=1,keepdim=True)[1] \n",
    "    # #print(output, labels)\n",
    "    # for i in range(0, len(val_dataloader)):\n",
    "      # #print(output[i][0].item())\n",
    "      # if output[i][0].item() == labels[i]:\n",
    "        # true = true+1\n",
    "        # tp[labels[i]] = tp[labels[i]]+1\n",
    "      # if output[i][0].item() != labels[i]:\n",
    "        # fp[output[i][0].item()] = fp[output[i][0].item()] + 1\n",
    "\n",
    "print(\"accuracy = \", true/total)\n",
    "for i in range(0, 42):\n",
    "  print(i, \" precision = \", (tp[i]/(tp[i]+fp[i]+0.00001)), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"C:/Users/Professional/Desktop/MOP/simps/kaggle_simpson_testset/kaggle_simpson_testset/comic_book_guy_11.jpg\")\n",
    "y, x = image.size\n",
    "trew = transforms.CenterCrop((min(x, y), min(x, y)))\n",
    "image = trew(image)\n",
    "image = (transform(image).unsqueeze(0))\n",
    "#del_digit(image)\n",
    "outputs = model(image.cuda())\n",
    "print(torch.argmax(outputs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e1f988cebd4bd2f8b7c87ce9ae61508ff27ed6ba25524cc848da380073580155"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
